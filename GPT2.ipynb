{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaping/anaconda3/envs/llama2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaping/anaconda3/envs/llama2/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Who are you? How much power do you control?\" He tried to look as soft as possible as he pulled away. \"Let\\'s see what you\\'re capable of...\" he replied, holding out his hand so she could see. \"If you do'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are you?\n",
      "\n",
      "I'm a man who has been in the military for over 30 years\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaping/anaconda3/envs/llama2/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Ensure CUDA is available\n",
    "assert torch.cuda.is_available(), \"CUDA is not available. Please check your CUDA installation.\"\n",
    "\n",
    "# Use a smaller model like distilGPT2 or gpt2-medium\n",
    "model_name = \"gpt2-medium\"  # or \"distilgpt2\" for an even smaller model\n",
    "\n",
    "# Load the pipeline with CUDA by setting the device to 0 (first GPU)\n",
    "pipe = pipeline(\"text-generation\", model=model_name, device=0)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# Generate a response using the pipeline\n",
    "response = pipe(messages[0][\"content\"])  # Adjusted to match pipeline input format\n",
    "print(response)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "# Prepare the input\n",
    "input_text = \"Who are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This project involves using a large language model to control a drone. The drone needs to follow detailed instructions, detect objects, and avoid obstacles while navigating through various environments. Describe the steps the drone should take to safely navigate through a dense forest while avoiding obstacles and identifying objects along the way. Also add features for additional functionality.\n",
      "\n",
      "The project's main goal is to find an efficient way to support multiple aircraft, to take this project to scale, and to make this simple to implement. I chose this approach as you'd expect for a basic drone app. It requires minimal code to generate the image, and to control other objects like servos, motors, batteries or LEDs without creating any global state. You'd expect things like this to be a bit easier in a more complex project. Even if there's less code, it's definitely cleaner than making things up. And, depending on your skills and interests, it can be a great way to get started with programming. Check the repository for more information and examples.\n",
      "\n",
      "The project's source code will be open sourced for anyone interested to contribute to. It's likely most notable contributors and supporters will be the team here at Codehaus.\n",
      "\n",
      "As always, the project is free for educational and research purpose. You may find the above code in the source file in any repository you choose. If you like the idea and think someone else might like the concept, please provide feedback in the Bugzilla.\n",
      "\n",
      "More in the series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This project involves using a large language model to control a drone. The drone needs to follow detailed instructions, detect objects, and avoid obstacles while navigating through various environments. Describe the steps the drone should take to safely navigate through a dense forest while avoiding obstacles and identifying objects along the way.\n",
      "\n",
      "The project is based on the following concepts:\n",
      "\n",
      "A large language model\n",
      "\n",
      "A large number of objects\n",
      "\n",
      "A large number of objects A large number of objects A large number of objects A large number of objects\n",
      "\n",
      "A large number of objects A large number of objects A large number of objects\n",
      "\n",
      "A large number of objects A large number of objects A large number of objects\n",
      "\n",
      "A large number of objects A large number of objects\n"
     ]
    }
   ],
   "source": [
    "# Define your contextual prompt\n",
    "context = \"This project involves using a large language model to control a drone. The drone needs to follow detailed instructions, detect objects, and avoid obstacles while navigating through various environments.\"\n",
    "prompt = \"Describe the steps the drone should take to safely navigate through a dense forest while avoiding obstacles and identifying objects along the way.\"\n",
    "\n",
    "# Combine context and prompt\n",
    "input_text = f\"{context} {prompt}\"\n",
    "\n",
    "# Generate a response using the pipeline\n",
    "response = pipe(input_text, max_length=500)  # Adjust max_length to control output length\n",
    "print(response[0]['generated_text'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "# Prepare the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(**inputs, max_length=150)\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A drone needs to navigate through a dense forest using a large language model. The drone must follow detailed instructions, avoid obstacles, and identify objects. Describe the steps it should take to ensure safe navigation. Additionally, suggest features to support multiple drones for scalability and ease of implementation.\n",
      "\n",
      "3. Understand the User Interface\n",
      "\n",
      "For this project, we are using Google SketchUp for a variety of projects. This is a great tool for any user-facing app that needs to communicate with other apps. We will use SketchUp to create a navigation bar and a simple icon. We will also create a basic user interface that allows the user to quickly navigate to different points in the map. We will also create a menu to allow the user to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaping/anaconda3/envs/llama2/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jiaping/anaconda3/envs/llama2/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A drone needs to navigate through a dense forest using a large language model. The drone must follow detailed instructions, avoid obstacles, and identify objects. Describe the steps it should take to ensure safe navigation. Additionally, suggest features to support multiple drones for scalability and ease of implementation.\n",
      "\n",
      "The following sections describe the steps required to implement a language model for a drone.\n",
      "\n",
      "The following sections describe the steps required to implement a language model for a drone.\n",
      "\n",
      "The following sections describe the steps required to implement a language model for a drone.\n",
      "\n",
      "The following sections describe the steps required to implement a language model for a drone.\n",
      "\n",
      "The following sections describe the steps required to implement a language model for a drone.\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Ensure CUDA is available\n",
    "assert torch.cuda.is_available(), \"CUDA is not available. Please check your CUDA installation.\"\n",
    "\n",
    "# Use a smaller model like distilGPT2 or gpt2-medium\n",
    "model_name = \"gpt2-medium\"  # or \"distilgpt2\" for an even smaller model\n",
    "\n",
    "# Load the pipeline with CUDA by setting the device to 0 (first GPU)\n",
    "pipe = pipeline(\"text-generation\", model=model_name, device=0)\n",
    "\n",
    "# Refined and specific prompt\n",
    "input_text = (\n",
    "    \"A drone needs to navigate through a dense forest using a large language model. \"\n",
    "    \"The drone must follow detailed instructions, avoid obstacles, and identify objects. \"\n",
    "    \"Describe the steps it should take to ensure safe navigation. Additionally, suggest features to support multiple drones for scalability and ease of implementation.\"\n",
    ")\n",
    "\n",
    "# Generate a response using the pipeline\n",
    "response = pipe(input_text, max_length=150, temperature=0.7, top_p=0.9)  # Adjusted parameters for better control\n",
    "print(response[0]['generated_text'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "# Prepare the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(**inputs, max_length=150, temperature=0.7, top_p=0.9)\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A drone needs to navigate through a lab using a large language model. The drone must follow instructions, avoid obstacles, and identify objects. Describe the steps it should take to ensure safe navigation.\n",
      "\n",
      "The best approach to learning a language is to use a small language model. This approach can be particularly helpful for people who are trying to learn a language with limited resources or language barriers.\n",
      "\n",
      "For example, if a person wants to learn a language, he or she could read and write a short paper in a language the person already understands. A small language model might include a sentence that explains the basic concepts of a language and a sentence that describes the rules of the language.\n",
      "\n",
      "For more information on learning a language, see Learning a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A drone needs to navigate through a lab using a large language model. The drone must follow instructions, avoid obstacles, and identify objects. Describe the steps it should take to ensure safe navigation.\n",
      "\n",
      "In this lesson, we will learn about the language model, a system that allows drones to understand and perform tasks.\n",
      "\n",
      "What is a Language Model?\n",
      "\n",
      "A language model is a system that provides a language to a drone. The language model is the set of rules for the drone to follow when performing tasks. The language model is the software that is used to communicate with the drone.\n",
      "\n",
      "The language model helps a drone follow instructions when it is flying. The drone uses the language model to navigate and control the drone. The drone\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Ensure CUDA is available\n",
    "assert torch.cuda.is_available(), \"CUDA is not available. Please check your CUDA installation.\"\n",
    "\n",
    "# Use a smaller model like distilGPT2 or gpt2-medium\n",
    "model_name = \"gpt2-medium\"  # or \"distilgpt2\" for an even smaller model\n",
    "\n",
    "# Load the pipeline with CUDA by setting the device to 0 (first GPU)\n",
    "pipe = pipeline(\"text-generation\", model=model_name, device=0)\n",
    "\n",
    "# Refined and specific prompt\n",
    "input_text = (\n",
    "    \"A drone needs to navigate through a lab using a large language model. \"\n",
    "    \"The drone must follow instructions, avoid obstacles, and identify objects. \"\n",
    "    \"Describe the steps it should take to ensure safe navigation.\"\n",
    ")\n",
    "\n",
    "# Generate a response using the pipeline\n",
    "response = pipe(input_text, max_length=150, do_sample=True, temperature=0.7, top_p=0.9, truncation=True)\n",
    "print(response[0]['generated_text'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "# Prepare the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to('cuda')\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(**inputs, max_length=150, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A drone needs to navigate through a lab using a large language model. The drone must follow detailed instructions, avoid obstacles, and identify objects. Describe the specific steps it should take to ensure safe and efficient navigation.\n",
      "\n",
      "Learn more about the Drone Safety Lab\n",
      "\n",
      "Learn more about how you can learn more about the drone safety lab.\n",
      "\n",
      "Learning About Drone Safety\n",
      "\n",
      "A drone needs to learn about the drone safety program. The drone must learn about the drone safety program.\n",
      "\n",
      "Learn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A drone needs to navigate through a lab using a large language model. The drone must follow detailed instructions, avoid obstacles, and identify objects. Describe the specific steps it should take to ensure safe and efficient navigation.\n",
      "\n",
      "The drone needs to navigate through a lab using a large language model. The drone must follow detailed instructions, avoid obstacles, and identify objects. Describe the specific steps it should take to ensure safe and efficient navigation. The drone needs to navigate through a lab using a large language\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Ensure CUDA is available\n",
    "assert torch.cuda.is_available(), \"CUDA is not available. Please check your CUDA installation.\"\n",
    "\n",
    "# Use a smaller model like distilGPT2 or gpt2-medium\n",
    "model_name = \"gpt2-medium\"  # or \"distilgpt2\" for an even smaller model\n",
    "\n",
    "# Load the pipeline with CUDA by setting the device to 0 (first GPU)\n",
    "pipe = pipeline(\"text-generation\", model=model_name, device=0)\n",
    "\n",
    "# Refined prompt with clear instructions\n",
    "input_text = (\n",
    "    \"A drone needs to navigate through a lab using a large language model. \"\n",
    "    \"The drone must follow detailed instructions, avoid obstacles, and identify objects. \"\n",
    "    \"Describe the specific steps it should take to ensure safe and efficient navigation.\"\n",
    ")\n",
    "\n",
    "# Generate a response using the pipeline\n",
    "response = pipe(input_text, max_length=100, do_sample=True, temperature=0.7, top_p=0.9, truncation=True)\n",
    "print(response[0]['generated_text'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model and move it to the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "# Prepare the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to('cuda')\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(**inputs, max_length=100, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.json',\n",
       " './saved_model/merges.txt',\n",
       " './saved_model/added_tokens.json',\n",
       " './saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After loading and potentially fine-tuning the model\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
